{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1"
      ],
      "metadata": {
        "id": "4E8SBcxSKtBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "\n",
        "# Given data\n",
        "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
        "\n",
        "# Number of observations\n",
        "n = len(x)\n",
        "\n",
        "# Compute regression coefficients\n",
        "X_mean = np.mean(x)\n",
        "Y_mean = np.mean(y)\n",
        "\n",
        "# Calculating the coefficients\n",
        "b1 = np.sum((x - X_mean) * (y - Y_mean)) / np.sum((x - X_mean) ** 2)\n",
        "b0 = Y_mean - b1 * X_mean\n",
        "\n",
        "# Predicting y values\n",
        "y_pred = b0 + b1 * x\n",
        "\n",
        "# Compute Sum of Squared Errors (SSE)\n",
        "SSE = np.sum((y - y_pred) ** 2)\n",
        "\n",
        "# Compute R^2 value\n",
        "SS_total = np.sum((y - Y_mean) ** 2)\n",
        "R2 = 1 - (SSE / SS_total)\n",
        "\n",
        "# Results\n",
        "print(f\"Intercept (b0): {b0.round(2)}\")\n",
        "print(f\"Slope (b1): {b1.round(2)}\")\n",
        "print(f\"Sum of Squared Errors (SSE): {SSE.round(2)}\")\n",
        "print(f\"R^2 value: {R2.round(2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TePtwgprX3NQ",
        "outputId": "e8ab3e8a-f95c-404b-a54a-f3c0cca9353d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept (b0): 1.24\n",
            "Slope (b1): 1.17\n",
            "Sum of Squared Errors (SSE): 5.62\n",
            "R^2 value: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries and given datasets\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Given data\n",
        "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)  # Reshape to 2D array\n",
        "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])"
      ],
      "metadata": {
        "id": "1PG62282eaQ8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full-batch Gradient Descent using LinearRegression from sklearn\n",
        "model_full_batch = LinearRegression()\n",
        "model_full_batch.fit(x, y)\n",
        "\n",
        "# Predicted values\n",
        "y_pred_full_batch = model_full_batch.predict(x)\n",
        "\n",
        "# Intercept and coefficient\n",
        "b0_full_batch_sklearn = model_full_batch.intercept_\n",
        "b1_full_batch_sklearn = model_full_batch.coef_[0]\n",
        "print(f\"Intercept (b0): {b0_full_batch_sklearn}\")\n",
        "print(f\"Coefficient (b1): {b1_full_batch_sklearn}\")\n",
        "\n",
        "# SSE from scratch for Full-batch Gradient Descent\n",
        "SSE_full_batch_sklearn = np.sum((y - y_pred_full_batch) ** 2)\n",
        "print(f\"SSE: {SSE_full_batch_sklearn}\")\n",
        "\n",
        "# R^2 score from scratch for Full-batch Gradient Descent\n",
        "SS_tot = np.sum((y - np.mean(y)) ** 2)\n",
        "R2_full_batch_sklearn = 1 - (np.sum((y - y_pred_full_batch) ** 2) / SS_tot)\n",
        "print(f\"R^2 Score: {R2_full_batch_sklearn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mtBkPsne4qS",
        "outputId": "010a798c-fb66-411f-9308-bf7f04c6bb5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept (b0): 1.2363636363636363\n",
            "Coefficient (b1): 1.1696969696969697\n",
            "SSE: 5.624242424242423\n",
            "R^2 Score: 0.952538038613988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "# Stochastic Gradient Descent using SGDRegressor from sklearn\n",
        "model_stochastic = SGDRegressor(max_iter=1000, tol=1e-6)\n",
        "model_stochastic.fit(x, y)\n",
        "\n",
        "# Predicted values for Stochastic Gradient Descent\n",
        "y_pred_stochastic_sklearn = model_stochastic.predict(x)\n",
        "\n",
        "# SSE from scratch for Stochastic Gradient Descent\n",
        "SSE_stochastic_sklearn = np.sum((y - y_pred_stochastic_sklearn) ** 2)\n",
        "print(f\"SSE (SGD): {SSE_stochastic_sklearn}\")\n",
        "\n",
        "# R^2 score from scratch for Stochastic Gradient Descent\n",
        "R2_stochastic_sklearn = 1 - (np.sum((y - y_pred_stochastic_sklearn) ** 2) / SS_tot)\n",
        "print(f\"R^2 Score (SGD): {R2_stochastic_sklearn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsJNYo50fGPx",
        "outputId": "e110e575-0784-41a3-da64-ffeec3768346"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSE (SGD): 6.444252077679657\n",
            "R^2 Score (SGD): 0.945618125926754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2"
      ],
      "metadata": {
        "id": "cIuRtvZzMMww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraires\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "\n",
        "# Define column names based on a typical housing dataset structure\n",
        "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "\n",
        "# Load the dataset with column names and the correct delimiter\n",
        "data = pd.read_csv('housing.csv', delim_whitespace=True, names=column_names)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Extract the correlation of each attribute with 'MEDV'\n",
        "correlation_with_medv = correlation_matrix['MEDV'].sort_values(ascending=False)\n",
        "\n",
        "# Display the correlation of each attribute with 'MEDV'\n",
        "print(\"Correlation of each attribute with MEDV (output price):\")\n",
        "print(correlation_with_medv)\n",
        "\n",
        "# Find the attribute with the highest correlation with 'MEDV'\n",
        "best_attribute = correlation_with_medv.idxmax()\n",
        "best_correlation = correlation_with_medv.max()\n",
        "\n",
        "print(f\"\\nThe attribute that best follows the linear relationship with MEDV is '{best_attribute}' with a correlation of {best_correlation:.2f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfLJ1pPuMTUz",
        "outputId": "147c0b27-c7ca-407b-eeac-21229d33bdfe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation of each attribute with MEDV (output price):\n",
            "MEDV       1.000000\n",
            "RM         0.695360\n",
            "ZN         0.360445\n",
            "B          0.333461\n",
            "DIS        0.249929\n",
            "CHAS       0.175260\n",
            "AGE       -0.376955\n",
            "RAD       -0.381626\n",
            "CRIM      -0.388305\n",
            "NOX       -0.427321\n",
            "TAX       -0.468536\n",
            "INDUS     -0.483725\n",
            "PTRATIO   -0.507787\n",
            "LSTAT     -0.737663\n",
            "Name: MEDV, dtype: float64\n",
            "\n",
            "The attribute that best follows the linear relationship with MEDV is 'MEDV' with a correlation of 1.00.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature matrix (X) and the response vector (y)\n",
        "X = data.drop('MEDV', axis=1).values  # Assuming 'MEDV' is the target variable\n",
        "y = data['MEDV'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
        "\n",
        "# Normalize the feature matrix X using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Add a column of ones for the intercept term\n",
        "X_train_b = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
        "X_test_b = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
        "\n",
        "# Compute the closed-form solution using the normal equation\n",
        "theta_analytic = np.linalg.inv(X_train_b.T.dot(X_train_b)).dot(X_train_b.T).dot(y_train)"
      ],
      "metadata": {
        "id": "6O1voWNhM06D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating gradient descent(Full-Batch)\n",
        "\n",
        "# Set parameters for gradient descent\n",
        "learning_rate = 0.001  # Lower learning rate to ensure convergence\n",
        "n_iterations = 1000\n",
        "m = X_train_b.shape[0]\n",
        "\n",
        "# Initialize theta (coefficients) with zeros\n",
        "theta_full_batch = np.zeros(X_train_b.shape[1])\n",
        "\n",
        "# Perform full-batch gradient descent\n",
        "for iteration in range(n_iterations):\n",
        "    gradients = 2/m * X_train_b.T.dot(X_train_b.dot(theta_full_batch) - y_train)\n",
        "    theta_full_batch -= learning_rate * gradients"
      ],
      "metadata": {
        "id": "YFbyL1FkM-Cs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SGDRegressor with a lower learning rate\n",
        "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3)\n",
        "\n",
        "# Fit the model to the standardized training data\n",
        "sgd_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the coefficients (including the intercept)\n",
        "theta_sgd = np.concatenate(([sgd_reg.intercept_[0]], sgd_reg.coef_))"
      ],
      "metadata": {
        "id": "JkatlYmENL5w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating columns and comparing results\n",
        "# Comparison of results\n",
        "df = pd.DataFrame({\n",
        "    \"Analytic Solution (Closed-Form)\": theta_analytic,\n",
        "    \"Full-Batch Gradient Descent\": theta_full_batch,\n",
        "    \"Stochastic Gradient Descent\": theta_sgd\n",
        "})\n",
        "\n",
        "# Print the DataFrame\n",
        "print(\"\\nComparison of coefficients:\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EJRylBGNT7N",
        "outputId": "d4e190d4-71bc-459a-b6c1-b9da6b2e606f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Comparison of coefficients:\n",
            "    Analytic Solution (Closed-Form)  Full-Batch Gradient Descent  \\\n",
            "0                         22.112541                    19.125921   \n",
            "1                         -0.796972                    -0.564164   \n",
            "2                          1.593507                     0.630986   \n",
            "3                          0.344417                    -0.456908   \n",
            "4                          0.590303                     0.805394   \n",
            "5                         -2.062413                    -0.638800   \n",
            "6                          2.442161                     2.900539   \n",
            "7                          0.058554                    -0.268819   \n",
            "8                         -2.924856                    -1.017952   \n",
            "9                          2.515060                     0.369254   \n",
            "10                        -2.063689                    -0.265583   \n",
            "11                        -1.819724                    -1.555737   \n",
            "12                         0.858254                     0.781418   \n",
            "13                        -3.731669                    -2.760108   \n",
            "\n",
            "    Stochastic Gradient Descent  \n",
            "0                     22.095485  \n",
            "1                     -0.687851  \n",
            "2                      1.275235  \n",
            "3                     -0.106616  \n",
            "4                      0.651295  \n",
            "5                     -1.833502  \n",
            "6                      2.581435  \n",
            "7                     -0.051615  \n",
            "8                     -2.654422  \n",
            "9                      1.483908  \n",
            "10                    -0.950941  \n",
            "11                    -1.827427  \n",
            "12                     0.885809  \n",
            "13                    -3.615656  \n"
          ]
        }
      ]
    }
  ]
}